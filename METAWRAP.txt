#maxbin2

#it found 2 bins, but i still think theres an error here:

########################################################################################################################
#####                                                RUNNING MAXBIN2                                               #####
########################################################################################################################


------------------------------------------------------------------------------------------------------------------------
-----                                          making contig depth file...                                         -----
------------------------------------------------------------------------------------------------------------------------

Output depth matrix to /mnt/data/lj752/data/MSc_training/scripting/data_output/metawrap/work_files/mb2_master_depth.txt
Calculating intra contig depth variance
Output matrix to /mnt/data/lj752/data/MSc_training/scripting/data_output/metawrap/work_files/mb2_master_depth.txt
Opening 5 bams
Consolidating headers
Processing bam files
Thread 0 finished: fastq_0.bam with 9565 reads and 241 readsWellMapped
Thread 1 finished: fastq_1.bam with 9654 reads and 240 readsWellMapped
Thread 2 finished: fastq_2.bam with 9646 reads and 217 readsWellMapped
Thread 4 finished: fastq_4.bam with 9571 reads and 201 readsWellMapped
Thread 3 finished: fastq_3.bam with 9632 reads and 241 readsWellMapped
Creating depth matrix file: /mnt/data/lj752/data/MSc_training/scripting/data_output/metawrap/work_files/mb2_master_depth.txt
Closing most bam files
Closing last bam file
Finished

------------------------------------------------------------------------------------------------------------------------
-----                    split master contig depth file into individual files for maxbin2 input                    -----
------------------------------------------------------------------------------------------------------------------------

processing fastq_0.bam depth file...
processing fastq_1.bam depth file...
processing fastq_2.bam depth file...
processing fastq_3.bam depth file...
processing fastq_4.bam depth file...
MaxBin 2.2.6
No Contig file. Please specify contig file by -contig
MaxBin - a metagenomics binning software.
Usage:
  run_MaxBin.pl
    -contig (contig file)
    -out (output file)

   (Input reads and abundance information)
    [-reads (reads file) -reads2 (readsfile) -reads3 (readsfile) -reads4 ... ]
    [-abund (abundance file) -abund2 (abundfile) -abund3 (abundfile) -abund4 ... ]

   (You can also input lists consisting of reads and abundance files)
    [-reads_list (list of reads files)]
    [-abund_list (list of abundance files)]

   (Other parameters)
    [-min_contig_length (minimum contig length. Default 1000)]
    [-max_iteration (maximum Expectation-Maximization algorithm iteration number. Default 50)]
    [-thread (thread num; default 1)]
    [-prob_threshold (probability threshold for EM final classification. Default 0.9)]
    [-plotmarker]
    [-markerset (marker gene sets, 107 (default) or 40.  See README for more information.)]

  (for debug purpose)
    [-version] [-v] (print version number)
    [-verbose]
    [-preserve_intermediate]

  Please specify either -reads or -abund information.
  You can input multiple reads and/or abundance files at the same time.
  Please read README file for more details.


#this is the output with ~100 files, if the error persisted (likely) I cannot scroll back up to see it :(
========== Job finished ==========
Yielded 6 bins for contig (scaffold) file /mnt/data/lj752/data/MSc_training/scripting/testing//work_files/assembly.fa

Here are the output files for this run.
Please refer to the README file for further details.

Summary file: /mnt/data/lj752/data/MSc_training/scripting/testing//work_files/maxbin2_out/bin.summary
Genome abundance info file: /mnt/data/lj752/data/MSc_training/scripting/testing//work_files/maxbin2_out/bin.abundance
Marker counts: /mnt/data/lj752/data/MSc_training/scripting/testing//work_files/maxbin2_out/bin.marker
Marker genes for each bin: /mnt/data/lj752/data/MSc_training/scripting/testing//work_files/maxbin2_out/bin.marker_of_each_gene.tar.gz
Bin files: /mnt/data/lj752/data/MSc_training/scripting/testing//work_files/maxbin2_out/bin.001.fasta - /mnt/data/lj752/data/MSc_training/scripting/testing//work_files/maxbin2_out/bin.006.fasta
Unbinned sequences: /mnt/data/lj752/data/MSc_training/scripting/testing//work_files/maxbin2_out/bin.noclass


========== Elapsed Time ==========
0 hours 1 minutes and 16 seconds.



#metabat

#does not produce anything, again i can't find the error message
#rn it with a small set to find the error, it doesn't say anything?? just doesn't run properly
########################################################################################################################
#####                                               RUNNING METABAT2                                               #####
########################################################################################################################


------------------------------------------------------------------------------------------------------------------------
-----                                          making contig depth file...                                         -----
------------------------------------------------------------------------------------------------------------------------

Output depth matrix to /mnt/data/lj752/data/MSc_training/scripting/testing/small/work_files/metabat_depth.txt
Output matrix to /mnt/data/lj752/data/MSc_training/scripting/testing/small/work_files/metabat_depth.txt
Opening 10 bams
Consolidating headers
Processing bam files
Thread 5 finished: fastq_61.bam with 10695 reads and 263 readsWellMapped
Thread 1 finished: fastq_12.bam with 10676 reads and 292 readsWellMapped
Thread 9 finished: fastq_79.bam with 10674 reads and 284 readsWellMapped
Thread 8 finished: fastq_78.bam with 10601 reads and 259 readsWellMapped
Thread 3 finished: fastq_34.bam with 10321 reads and 253 readsWellMapped
Thread 4 finished: fastq_50.bam with 10409 reads and 252 readsWellMapped
Thread 2 finished: fastq_2.bam with 10803 reads and 313 readsWellMapped
Thread 6 finished: fastq_72.bam with 10578 reads and 292 readsWellMapped
Thread 0 finished: fastq_1.bam with 10691 reads and 331 readsWellMapped
Thread 7 finished: fastq_73.bam with 10650 reads and 308 readsWellMapped
Creating depth matrix file: /mnt/data/lj752/data/MSc_training/scripting/testing/small/work_files/metabat_depth.txt
Closing most bam files
Closing last bam file
Finished

------------------------------------------------------------------------------------------------------------------------
-----                                       Starting binning with metaBAT2...                                      -----
------------------------------------------------------------------------------------------------------------------------

MetaBAT 2 (v2.12.1) using minContig 1500, minCV 1.0, minCVSum 1.0, maxP 95%, minS 60, and maxEdges 200. 
0 bins (0 bases in total) formed.

------------------------------------------------------------------------------------------------------------------------
-----                               metaBAT2 finished successfully, and found 0 bins!                              -----
------------------------------------------------------------------------------------------------------------------------




#concoct

------------------------------------------------------------------------------------------------------------------------
-----                                    estimating contig fragment coverage...                                    -----
------------------------------------------------------------------------------------------------------------------------

/mnt/data/lj752/tools/miniforge/envs/metawrap_env/bin/concoct_coverage_table.py:61: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\t'.
  df = pd.read_table(fh, header=None)
usage: concoct [-h] [--coverage_file COVERAGE_FILE]
               [--composition_file COMPOSITION_FILE] [-c CLUSTERS]
               [-k KMER_LENGTH] [-t THREADS] [-l LENGTH_THRESHOLD]
               [-r READ_LENGTH] [--total_percentage_pca TOTAL_PERCENTAGE_PCA]
               [-b BASENAME] [-s SEED] [-i ITERATIONS]
               [--no_cov_normalization] [--no_total_coverage]
               [--no_original_data] [-o] [-d] [-v]

optional arguments:
  -h, --help            show this help message and exit
  --coverage_file COVERAGE_FILE
                        specify the coverage file, containing a table where
                        each row correspond to a contig, and each column
                        correspond to a sample. The values are the average
                        coverage for this contig in that sample. All values
                        are separated with tabs.
  --composition_file COMPOSITION_FILE
                        specify the composition file, containing sequences in
                        fasta format. It is named the composition file since
                        it is used to calculate the kmer composition (the
                        genomic signature) of each contig.
  -c CLUSTERS, --clusters CLUSTERS
                        specify maximal number of clusters for VGMM, default
                        400.
  -k KMER_LENGTH, --kmer_length KMER_LENGTH
                        specify kmer length, default 4.
  -t THREADS, --threads THREADS
                        Number of threads to use
  -l LENGTH_THRESHOLD, --length_threshold LENGTH_THRESHOLD
                        specify the sequence length threshold, contigs shorter
                        than this value will not be included. Defaults to
                        1000.
  -r READ_LENGTH, --read_length READ_LENGTH
                        specify read length for coverage, default 100
  --total_percentage_pca TOTAL_PERCENTAGE_PCA
                        The percentage of variance explained by the principal
                        components for the combined data.
  -b BASENAME, --basename BASENAME
                        Specify the basename for files or directory where
                        outputwill be placed. Path to existing directory or
                        basenamewith a trailing '/' will be interpreted as a
                        directory.If not provided, current directory will be
                        used.
  -s SEED, --seed SEED  Specify an integer to use as seed for clustering. 0
                        gives a random seed, 1 is the default seed and any
                        other positive integer can be used. Other values give
                        ArgumentTypeError.
  -i ITERATIONS, --iterations ITERATIONS
                        Specify maximum number of iterations for the VBGMM.
                        Default value is 500
  --no_cov_normalization
                        By default the coverage is normalized with regards to
                        samples, then normalized with regards of contigs and
                        finally log transformed. By setting this flag you skip
                        the normalization and only do log transorm of the
                        coverage.
  --no_total_coverage   By default, the total coverage is added as a new
                        column in the coverage data matrix, independently of
                        coverage normalization but previous to log
                        transformation. Use this tag to escape this behaviour.
  --no_original_data    By default the original data is saved to disk. For big
                        datasets, especially when a large k is used for
                        compositional data, this file can become very large.
                        Use this tag if you don't want to save the original
                        data.
  -o, --converge_out    Write convergence info to files.
  -d, --debug           Debug parameters.
  -v, --version         show program's version number and exit

------------------------------------------------------------------------------------------------------------------------
-----                                       Starting binning with CONCOCT...                                       -----
------------------------------------------------------------------------------------------------------------------------

WARNING:root:CONCOCT is running in single threaded mode. Please, consider adjusting the --threads parameter.
Up and running. Check /mnt/data/lj752/data/MSc_training/scripting/testing/work_files/concoct_out/log.txt for progress
/mnt/data/lj752/tools/miniforge/envs/metawrap_env/lib/python2.7/site-packages/concoct/input.py:82: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\t'.
  cov = p.read_table(cov_file, header=0, index_col=0)


#concoct found 7 bins, similar to maxbin2 so im happy its working but why is this error persisting?


#total time for ~100 samples
real	186m25.462s
user	195m54.340s
sys	1m32.075s


