#!/bin/bash

# Path to the configuration file
config_file="$(pwd)/config.ini"

# Function to read a value from the configuration file
get_config_value() {
    awk -F "=" -v section="$1" -v key="$2" '$0 ~ "\\["section"\\]" {flag=1; next} /\\[.*\\]/ {flag=0} flag && $1 ~ key {print $2; exit}' "$config_file" | tr -d ' '
}


#Define file path
output_data_dir=$(get_config_value "Data" "output_data_dir")
DATA_OUTPUT_PATH=${output_data_dir/CURRENT_DIR/$(pwd)}
mkdir -p "$DATA_OUTPUT_PATH"

#define progress log + function
PROGRESS_FILE="$DATA_OUTPUT_PATH/progress.log"
log_progress() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$PROGRESS_FILE"
}

log_progress "Starting the pipeline..."
echo "If not using defaults, define paths and program options in the config file. Checkpoints are used to skip completed steps. "
echo "Note: If you wish to rerun the entire pipeline or specific steps, please delete the '.<step_name>_done' files from '$DATA_OUTPUT_PATH'. with 'rm -f .<step>_done'"
echo "To delete all checkpoint files and restart, run: 'rm $DATA_OUTPUT_PATH/.*_done'"
echo "Using output data directory: $DATA_OUTPUT_PATH"


# List of programs to check
declare -A program_paths
declare -a missing_default_programs
default_programs=("Dorado" "Nanoplot" "Flye" "Meryl" "Merqury" "Medaka" "Metawrap" "Kraken2" "Bracken")

# Function to check program paths
check_programs() {
    local program_list=("${!1}")
    for program in "${program_list[@]}"; do
        local program_path=""
        # Check standard locations first
        if [ -x "/usr/local/bin/$program" ]; then
            program_path="/usr/local/bin/$program"
        elif [ -x "/usr/bin/$program" ]; then
            program_path="/usr/bin/$program"
        else
            # If not found in standard locations, check the configuration file
            program_path=$(get_config_value "$program" "path")
        fi

        if [ -n "$program_path" ]; then
            program_paths[$program]="$program_path"
            echo "Using $program at: $program_path"
        else
            missing_default_programs+=("$program")
        fi
    done
}

# Check default programs
check_programs default_programs[@]

# Handle missing default programs
if [ ${#missing_default_programs[@]} -ne 0 ]; then
    echo "The following default programs were not found: ${missing_default_programs[*]}."
    echo "Please specify the paths to these programs in the configuration file and rerun the script."
    exit 1
fi



# Prompt the user for the input file location
echo "Please enter the path to your input data directory:"
read INPUT_PATH

if [[ "$INPUT_PATH" == */ ]]; then
    INPUT_PATH="${INPUT_PATH%?}"
fi

# Check if the file exists
if [ -d "$INPUT_PATH" ]; then
    echo "Directory found at: $INPUT_PATH"
else
    echo "Directory not found at: $INPUT_PATH. Please check the path and try again."
    exit
fi

#Prompt the user for the input file type
echo "Which input file type are you using? (pod5/fast5/fastq/fastq.gz)"
read FILE_TYPE

# Define file paths
VIRTUAL_ENV_PATH=$(get_config_value "Medaka" "medaka_env_path")
PYTHON_PATH=$(which python || which python3)

#Functions to create pipeline checkpoints 
create_checkpoint() {
    touch "$DATA_OUTPUT_PATH/.${1}_done"
}
check_for_checkpoint() {
    if [ -f "$DATA_OUTPUT_PATH/.${1}_done" ]; then
        echo "Checkpoint for ${1} found. Skipping..."
        return 0 
    else
        return 1
    fi
}

# Create directories for each tool output within the virtual environment
mkdir -p $DATA_OUTPUT_PATH/dorado
mkdir -p $DATA_OUTPUT_PATH/nanoplot
mkdir -p $DATA_OUTPUT_PATH/flye
mkdir -p $DATA_OUTPUT_PATH/meryl
mkdir -p $DATA_OUTPUT_PATH/merqury
mkdir -p $DATA_OUTPUT_PATH/medaka
mkdir -p $DATA_OUTPUT_PATH/metawrap
mkdir -p $DATA_OUTPUT_PATH/kraken2
mkdir -p $DATA_OUTPUT_PATH/bracken


 # Ask user if they want to run Dorado for base calling
read -p "Do you want to run Dorado for base calling? (yes/no): " run_dorado
if ! check_for_checkpoint "dorado"; then
    if [[ "$run_dorado" == "yes" ]]; then
        log_progress "Starting Dorado analysis..."
        cd $DATA_OUTPUT_PATH/dorado
        export PATH=${program_paths[Dorado]}:$PATH
        DORADO_MODEL=$(get_config_value "Dorado" "default_model")
        dorado download --model $DORADO_MODEL
        dorado basecaller $DORADO_MODEL $INPUT_PATH/ > calls.fastq --emit-fastq
        create_checkpoint "dorado"
        log_progress "Dorado analysis completed. Results are stored in $DATA_OUTPUT_PATH/dorado"
    fi
fi
if ! check_for_checkpoint "nanoplot"; then
	log_progress "Running NanoPlot..."
	export PATH=${program_paths[NanoPlot]}:$PATH
	THREADS=$(get_config_value "Data" "default_threads")
	    if [[ "$run_dorado" == "yes" ]]; then
	    	${program_paths[Nanoplot]} --fastq $DATA_OUTPUT_PATH/dorado/calls.fastq --outdir $DATA_OUTPUT_PATH/nanoplot --threads $THREADS
	    else
	    	${program_paths[Nanoplot]} --fastq $INPUT_PATH/*$FILE_TYPE --outdir $DATA_OUTPUT_PATH/nanoplot --threads $THREADS
	    fi
	log_progress "Nanoplot analysis completed. Results are stored in $DATA_OUTPUT_PATH/nanoplot"
	create_checkpoint "nanoplot"
fi

if ! check_for_checkpoint "flye"; then
    log_progress "Starting Flye analysis ..."
    export PATH=${program_paths[Flye]}:$PATH
    FLYE_ITERATIONS=$(get_config_value "Flye" "default_iterations")
    FLYE_QUALITY=$(get_config_value "Flye" "default_read_quality")
    THREADS=$(get_config_value "Data" "default_threads")
    if [[ "$run_dorado" == "yes" ]]; then
        ${PYTHON_PATH} ${program_paths[Flye]} --nano-hq $DATA_OUTPUT_PATH/dorado/calls.fastq --out-dir $DATA_OUTPUT_PATH/flye --iterations $FLYE_ITERATIONS --meta --threads $THREADS
    else
        ${PYTHON_PATH} ${program_paths[Flye]} --nano-hq $INPUT_PATH/*$FILE_TYPE --out-dir $DATA_OUTPUT_PATH/flye --iterations $FLYE_ITERATIONS --meta --threads $THREADS
    fi
rm -r $DATA_OUTPUT_PATH/flye/00-assembly
rm -r $DATA_OUTPUT_PATH/flye/10-consensus
rm -r $DATA_OUTPUT_PATH/flye/20-repeat
rm -r $DATA_OUTPUT_PATH/flye/30-contigger
create_checkpoint "flye"
log_progress "Flye analysis completed. Results are stored in $DATA_OUTPUT_PATH/flye"
fi

if ! check_for_checkpoint "meryl"; then
    log_progress "Starting Meryl analysis..."
    cd $DATA_OUTPUT_PATH/meryl
    export PATH=${program_paths[Meryl]}:$PATH
    MERYL_KMERS=$(get_config_value "Meryl" "default_kmers")
    THREADS=$(get_config_value "Data" "default_threads")
    MEMORY=$(get_config_value "Data" "default_memory")
    meryl count k=$MERYL_KMERS $DATA_OUTPUT_PATH/flye/assembly.fasta output assembly.k$MERYL_KMERS.meryl threads=$THREADS memory=$MEMORY
    create_checkpoint "meryl"
    log_progress "Meryl analysis completed. Results are stored in $DATA_OUTPUT_PATH/meryl"
fi
if ! check_for_checkpoint "merqury"; then
    log_progress "Starting Merqury analysis..."
    cd $DATA_OUTPUT_PATH/merqury
    export PATH=${program_paths[Merqury]}:$PATH
    export MERQURY=${program_paths[Merqury]}
    merqury.sh $DATA_OUTPUT_PATH/meryl/assembly.k$MERYL_KMERS.meryl $DATA_OUTPUT_PATH/flye/assembly.fasta merqury_output
    create_checkpoint "merqury"
    log_progress "Merqury analysis completed. Results are stored in $DATA_OUTPUT_PATH/merqury"
fi

        
if ! check_for_checkpoint "medaka"; then
    log_progress "Starting Medaka analysis..."
    source $VIRTUAL_ENV_PATH
    export PATH=${program_paths[Medaka]}:$PATH
    THREADS=$(get_config_value "Data" "default_threads")
    MODEL=$(get_config_value "Medaka" "default_model")
    ITERATIONS=$(get_config_value "Medaka" "default_iterations")
    TEMP_FASTQ_PATH="$INPUT_PATH/temp"
    if [[ "$run_dorado" == "yes" ]]; then
        medaka_consensus -i $DATA_OUTPUT_PATH/dorado/calls.fastq -d $DATA_OUTPUT_PATH/flye/assembly.fasta -o $DATA_OUTPUT_PATH/medaka/work_files -t $THREADS -m $MODEL
    elif [[ "$FILE_TYPE" == "fastq.gz" ]]; then
        mkdir -p "$TEMP_FASTQ_PATH"
        for gz in $INPUT_PATH/*.fastq.gz; do
            gzip -dkc "$gz" > "$TEMP_FASTQ_PATH/$(basename "${gz%.*}")"
        done
    	medaka_consensus -i $TEMP_FASTQ_PATH -d $DATA_OUTPUT_PATH/flye/assembly.fasta -o $DATA_OUTPUT_PATH/medaka/work_files -t $THREADS -m $MODEL
    else
        medaka_consensus -i $INPUT_PATH -d $DATA_OUTPUT_PATH/flye/assembly.fasta -o $DATA_OUTPUT_PATH/medaka/work_files -t $THREADS -m $MODEL
    fi
    log_progress "Starting second Medaka iteration..."
    medaka_consensus -i $DATA_OUTPUT_PATH/medaka/work_files/consensus.fasta -d $DATA_OUTPUT_PATH/flye/assembly.fasta -o $DATA_OUTPUT_PATH/medaka
    rm -r $DATA_OUTPUT_PATH/medaka/work_files
    create_checkpoint "medaka"
    log_progress "Medaka analysis completed. Results are stored in $DATA_OUTPUT_PATH/medaka"
    deactivate
fi  


if ! check_for_checkpoint "metawrap"; then
    log_progress "Starting MetaWrap analysis..."
    eval "$(conda shell.bash hook)"
    METAWRAP_ENV=$(get_config_value "Metawrap" "conda_env")
    if [ -z "$METAWRAP_ENV" ]; then
        echo "Conda environment name not found in config.ini. Please specify it under the [Metawrap] section."
        exit 1
    else
        log_progress "Using Conda environment: $METAWRAP_ENV"
    fi
    conda activate $METAWRAP_ENV
    export PATH=${program_paths[Metawrap]}:$PATH
    THREADS=$(get_config_value "Data" "default_threads")
    TEMP_FASTQ_PATH="$INPUT_PATH/temp"
    META_FASTQ_FILES=$(find "$INPUT_PATH" -type f -name "*.fastq" | paste -sd " " -)
    META_TEMP_FASTQ_FILES=$(find "$TEMP_FASTQ_PATH" -type f -name "*.fastq" | paste -sd " " -)
    META_BASE_CMD="metawrap binning -o $DATA_OUTPUT_PATH/metawrap -a $DATA_OUTPUT_PATH/medaka/consensus.fasta --maxbin2 --concoct --single-end"
    META_FASTQ_CMD="$META_BASE_CMD $META_FASTQ_FILES"
    META_TEMP_FASTQ_CMD="$META_BASE_CMD $META_TEMP_FASTQ_FILES"
    if [[ "$run_dorado" == "yes" ]]; then
        $META_BASE_CMD $DATA_OUTPUT_PATH/dorado/calls.fastq -t $THREADS
    elif [[ "$FILE_TYPE" == "fastq.gz" ]]; then
        $META_TEMP_FASTQ_CMD $META_TEMP_FASTQ_FILES -t $THREADS
    else
        $META_FASTQ_CMD $META_FASTQ_FILES -t $THREADS
    fi
    metawrap bin_refinement -o $DATA_OUTPUT_PATH/metawrap/final_bins -A $DATA_OUTPUT_PATH/metawrap/concoct_bins -B $DATA_OUTPUT_PATH/metawrap/maxbin2_bins -t $THREADS
    conda deactivate
    rm -r $DATA_OUTPUT_PATH/metawrap/work_files
    rm -r $DATA_OUTPUT_PATH/metawrap/concoct_bins
    rm -r $DATA_OUTPUT_PATH/metawrap/maxbin2_bins
    mkdir $DATA_OUTPUT_PATH/metawrap/stats
    find $DATA_OUTPUT_PATH/metawrap/final_bins/work_files -maxdepth 1 -name "*.stats" -exec mv {} $DATA_OUTPUT_PATH/metawrap/stats \;
    rm -r $DATA_OUTPUT_PATH/metawrap/final_bins/work_files
    find $DATA_OUTPUT_PATH/metawrap/final_bins -maxdepth 1 -name "*.stats" -exec mv {} $DATA_OUTPUT_PATH/metawrap/stats \;
    find $DATA_OUTPUT_PATH/metawrap/final_bins -maxdepth 1 -name "*.contigs" -exec mv {} $DATA_OUTPUT_PATH/metawrap/stats \;
    create_checkpoint "metawrap"
    log_progress "Metawrap analysis completed. Results are stored in $DATA_OUTPUT_PATH/metawrap"
fi
    
#Kraken2 

if ! check_for_checkpoint "kraken2"; then
    DBNAME=$(get_config_value "Kraken2" "dbname")
    THREADS=$(get_config_value "Data" "default_threads")
    database_setup_success=false
    database_setup_manual=false
    if [[ -z "$DBNAME" ]]; then
        read -p "No Kraken2 database location found in the config. Do you have an existing Kraken2 database? (yes/no): " has_kraken_db
        if [[ "$has_kraken_db" == "yes" ]]; then
            read -p "Enter the location of your existing Kraken2 database: " DBNAME
	database_setup_success=true
	database_setup_manual=false
        else
            read -p "Enter a location for your new Kraken2 database: " DBNAME
            log_progress "Creating and building new Kraken2 database at $DBNAME..."
	mkdir -p "$DBNAME"
            if kraken2-build --standard --db "$DBNAME" --threads $THREADS | kraken2-build --db "$DBNAME" --download-taxonomy ; then
	    database_setup_success=true
	database_setup_manual=false
      	    else
                log_progress "Standard database build failed. Attempting to download prebuilt database..."
                wget -O "$DBNAME/k2_standard_20240112.tar.gz" https://genome-idx.s3.amazonaws.com/kraken/k2_standard_20240112.tar.gz
		if [ $? -eq 0 ]; then
                    log_progress "Extracting database..."
		    tar -xzf "$DBNAME/k2_standard_20240112.tar.gz" -C "$DBNAME" --strip-components=1 
		    rm "$DBNAME/k2_standard_20240112.tar.gz"
      		    database_setup_success=true
		    database_setup_manual=true
                else
                    log_progress "Failed to download the prebuilt database. Please check your internet connection or the URL and try again."
                    fi
                fi
	    fi 
	    if [[ "$database_setup_success" == true ]]; then
                log_progress "Database $DBNAME setup complete."
                sed -i "/^\[Kraken2\]/,/^\[/ {/^dbname=/ s|=.*|=$DBNAME|}" "$config_file"
                log_progress "Kraken2 database location updated in config: $DBNAME"
            else
	    	log_progress "Kraken2 database setup failed. Exiting."
      		exit 1
    	    fi
        else
   	    log_progress "Found Kraken2 database location in config: $DBNAME"
	    database_setup_success=true
        fi
	if [[ "$database_setup_success" == true ]]; then
            log_progress "Starting  Kraken2 analysis..."
	    EXCLUDE_DIRS=("concoct_bins" "figures" "maxbin2_bins" "work_files")
 	    UNIQUE_DIRS=$(find "$DATA_OUTPUT_PATH/metawrap/final_bins" -mindepth 1 -maxdepth 1 -type d | grep -vE "$(printf "|%s" "${EXCLUDE_DIRS[@]}" | sed 's/^|//')")
    	    if [ -z "$UNIQUE_DIRS" ]; then
           	log_progress "No unique directories found."
            	exit 1
   	    fi
    	    log_progress "Unique directories found: $UNIQUE_DIRS"
    	    while read -r UNIQUE_DIR; do
       	    	if [ ! -z "$UNIQUE_DIR" ]; then
            	    log_progress "Processing directory: $UNIQUE_DIR"
            	    FASTA_FILES=$(find "$UNIQUE_DIR" -type f -name "*.fa")
            	    if [ -z "$FASTA_FILES" ]; then
                    	log_progress "No FASTA files found in $UNIQUE_DIR."
                    	continue
            	    fi
            	    while read -r FASTA_FILE; do
                    	log_progress "Running Kraken2 analysis on $FASTA_FILE..."
                    	basename_fasta=$(basename "$FASTA_FILE" .fa)
                    	kraken2 --db "$DBNAME" "$FASTA_FILE" --output "$DATA_OUTPUT_PATH/kraken2/${basename_fasta}.kraken" --report "$DATA_OUTPUT_PATH/kraken2/${basename_fasta}_kraken2_report.txt" --threads $THREADS --confidence 0.01 --minimum-base-quality 0
            	    done <<< "$FASTA_FILES"
            	fi
    	    	done <<< "$UNIQUE_DIRS"
    	    else
            	log_progress "Unable to proceed without a valid Kraken2 database setup."
            exit 1
    	fi
    	create_checkpoint "kraken2"
	log_progress "Kraken2 analysis completed. Results are stored in $DATA_OUTPUT_PATH/metawrap"
fi


if ! check_for_checkpoint "bracken"; then
    eval "$(conda shell.bash hook)"
    BRACKEN_ENV=$(get_config_value "Bracken" "conda_env")
    export PATH=${program_paths[Bracken]}:$PATH
    if [ -z "$BRACKEN_ENV" ]; then
        echo "Conda environment name not found in config.ini. Please specify it under the [Bracken] section."
        exit 1
    fi
    echo "Using Conda environment: $BRACKEN_ENV"
    conda activate $BRACKEN_ENV
    log_progress "Starting Bracken analysis..."
    DBNAME=$(get_config_value "Kraken2" "dbname")
    THREADS=$(get_config_value "Data" "default_threads")
    READ_LEN=$(get_config_value "Bracken" "read_length")
    if [[ "$database_setup_manual" == "false" ]]; then
    	log_progress "Building Bracken database using $DBNAME..."
    	bracken-build -d $DBNAME  -t $THREADS -l $READ_LEN
    	log_progress "Starting Bracken analysis..."
        find "$DATA_OUTPUT_PATH/kraken2/" -type f -name "*.report.txt" | while read report_file; do
    	    base_report=$(basename "$REPORT" "report.txt")
	    bracken -d $DBNAME -i $REPORT -o $DATA_OUTPUT_PATH/braken/${base_report}.txt -l S -t $THREADS -r $READ_LEN
	    log_progress "Bracken analysis completed for $base_report..."
        done
    else
    	find "$DATA_OUTPUT_PATH/kraken2/" -type f -name "*.report.txt" | while read report_file; do
    	    base_report=$(basename "$REPORT" "report.txt")
    	    python est_abundance.py -i $REPORT -k $DBNAME/database${READ_LEN}mers.kmer_distrib -o $DATA_OUTPUT_PATH/braken/${base_report}.txt -l S -t $THREADS
    	done
    fi
    create_checkpoint "bracken"
    log_progress "Bracken analysis completed. Results are stored in $DATA_OUTPUT_PATH/bracken"
    conda deactivate
fi


log_progress "Pipeline execution completed."
echo "If you wish to rerun the pipeline or specific steps, remember to delete the '.<step_name>_done' checkpoint files from '$DATA_OUTPUT_PATH'."

